{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - Frozen Lake\n",
    "\n",
    "- Brando Reyes\n",
    "- Juan Pablo Solis\n",
    "- Carlos Valladares\n",
    "\n",
    "# Enlace del Repositorio:\n",
    "https://github.com/vgcarlol/Corto1_IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerías a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación del entorno de FrozekLake con slippery=True (Hielo resbaladizo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del agente\n",
    "taza_aprendizaje = 0.1  # Tasa de aprendizaje\n",
    "gamma = 0.99  # Factor de descuento\n",
    "epsilon = 1.0  # Probabilidad inicial de exploración\n",
    "epsilon_decay = 0.995  # Factor de decaimiento de epsilon\n",
    "epsilon_min = 0.01  # Valor mínimo de epsilon\n",
    "num_episodios = 10000  # Número de episodios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos la tabla Q con ceros\n",
    "tablaQ = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para elegir una acción con política epsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elegir_accion(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Exploración\n",
    "    else:\n",
    "        return np.argmax(tablaQ[state, :])  # Explotación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 0 - Epsilon: 0.010\n",
      "Episodio 1000 - Epsilon: 0.010\n",
      "Episodio 2000 - Epsilon: 0.010\n",
      "Episodio 3000 - Epsilon: 0.010\n",
      "Episodio 4000 - Epsilon: 0.010\n",
      "Episodio 5000 - Epsilon: 0.010\n",
      "Episodio 6000 - Epsilon: 0.010\n",
      "Episodio 7000 - Epsilon: 0.010\n",
      "Episodio 8000 - Epsilon: 0.010\n",
      "Episodio 9000 - Epsilon: 0.010\n",
      "Entrenamiento terminado.\n"
     ]
    }
   ],
   "source": [
    "# Bucle de entrenamiento\n",
    "for episode in range(num_episodios):\n",
    "    estado, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        accion = elegir_accion(estado)\n",
    "        siguiente_estado, recompenza, done, _, _ = env.step(accion)\n",
    "        \n",
    "        # Actualización de la tabla Q con ecuación de Q-learning\n",
    "        tablaQ[estado, accion] = tablaQ[estado, accion] + taza_aprendizaje * (\n",
    "            recompenza + gamma * np.max(tablaQ[siguiente_estado, :]) - tablaQ[estado, accion]\n",
    "        )\n",
    "        \n",
    "        estado = siguiente_estado\n",
    "    \n",
    "    # Reducir epsilon para menos exploración con el tiempo\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    \n",
    "    # Mostrar progreso cada 1000 episodios\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episodio {episode} - Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "print(\"Entrenamiento terminado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 80.00%\n",
      "Recompensa media por episodio: 0.80\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del agente\n",
    "def evaluate_agent(num_tests=100):\n",
    "    success_count = 0\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(tablaQ[state, :])  # Elegir la mejor acción\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        \n",
    "        total_rewards += episode_reward\n",
    "        if episode_reward > 0:  # Si llegó a la meta\n",
    "            success_count += 1\n",
    "    \n",
    "    success_rate = success_count / num_tests * 100\n",
    "    avg_reward = total_rewards / num_tests\n",
    "    \n",
    "    print(f\"Tasa de éxito: {success_rate:.2f}%\")\n",
    "    print(f\"Recompensa media por episodio: {avg_reward:.2f}\")\n",
    "\n",
    "# Ejecutar evaluación\n",
    "evaluate_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendiente:\n",
    "\n",
    "- Generación de un nuevo mapa aleatorio en cada ejecución.\n",
    "- Guardar y cargar la tabla Q para reutilización.\n",
    "- Optimización de hiperparámetros (learning_rate, gamma, epsilon_decay).\n",
    "- Visualización del entrenamiento (gráficos de desempeño).\n",
    "- Renderizado del juego para ver cómo juega el agente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
